<!DOCTYPE html>
<html >
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
      <title>Vocal Transcription</title>
    
      <link rel="stylesheet" href="../_static/pygments.css">
      <link rel="stylesheet" href="../_static/theme.css">
      <link rel="stylesheet" href="../_static/sphinx_press_theme.css">
          <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
      
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>

      <!-- sphinx script_files -->
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

      
      <script src="../_static/theme-vendors.js"></script>
      <script src="../_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Vocal-Contour Transcription" href="../vocal-contour/api.html" />
  <link rel="prev" title="Chord Transcription" href="../chord/api.html" /> 
  </head>

  <body>
    <div id="app" class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../index.html" class="home-link">
    
      <span class="site-name">omnizart</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">

  
    <div class="nav-item">
      <a href="../index.html#sound-samples"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="../index.html#sound-samples"
         class="nav-link ">
         Command Line Interface
      </a>
    </div>
  
    <div class="nav-item">
      <a href="../index.html#sound-samples"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            

  
    <div class="nav-item">
      <a href="../index.html#sound-samples"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="../index.html#sound-samples"
         class="nav-link ">
         Command Line Interface
      </a>
    </div>
  
    <div class="nav-item">
      <a href="../index.html#sound-samples"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../index.html#sound-samples">Contents</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 "><a href="../quick-start.html" class="reference internal ">Quick Start</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../tutorial.html" class="reference internal ">Tutorial</a>

            
          </li>

        
      </ul>
    </div>
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../index.html#sound-samples">Command Line Interface</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 "><a href="../music/cli.html" class="reference internal ">omnizart music</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../drum/cli.html" class="reference internal ">omnizart drum</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../chord/cli.html" class="reference internal ">omnizart chord</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="cli.html" class="reference internal ">omnizart vocal</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../vocal-contour/cli.html" class="reference internal ">omnizart vocal-contour</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../beat/cli.html" class="reference internal ">omnizart beat</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../patch-cnn/cli.html" class="reference internal ">omnizart patch-cnn</a>

            
          </li>

        
      </ul>
    </div>
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../index.html#sound-samples">API Reference</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 "><a href="../music/api.html" class="reference internal ">Music Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../drum/api.html" class="reference internal ">Drum Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../chord/api.html" class="reference internal ">Chord Transcription</a>

            
          </li>

        
          <li class="toctree-l1 current"><a href="#" class="reference internal current">Vocal Transcription</a>

            
              <ul>
                
                  <li class="toctree-l2"><a href="#feature-storage-format" class="reference internal">Feature Storage Format</a></li>
                
                  <li class="toctree-l2"><a href="#app" class="reference internal">App</a></li>
                
                  <li class="toctree-l2"><a href="#dataset" class="reference internal">Dataset</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.vocal.inference" class="reference internal">Inference</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.vocal.labels" class="reference internal">Labels</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.vocal.prediction" class="reference internal">Prediction</a></li>
                
                  <li class="toctree-l2"><a href="#settings" class="reference internal">Settings</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 "><a href="../vocal-contour/api.html" class="reference internal ">Vocal-Contour Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../patch-cnn/api.html" class="reference internal ">Patch-CNN Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../beat/api.html" class="reference internal ">Beat Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../feature.html" class="reference internal ">Feature</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../models.html" class="reference internal ">Models</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../training.html" class="reference internal ">Training</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../base.html" class="reference internal ">Base Classes</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../constants.html" class="reference internal ">Constants</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="../utils.html" class="reference internal ">Utilities</a>

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
    
    <li>Vocal Transcription</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="../chord/api.html"
       title="previous chapter">← Chord Transcription</a>
  </li>
  <li class="next">
    <a href="../vocal-contour/api.html"
       title="next chapter">Vocal-Contour Transcription →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main">
            
  <div class="section" id="module-omnizart.vocal">
<span id="vocal-transcription"></span><h1>Vocal Transcription<a class="headerlink" href="#module-omnizart.vocal" title="Permalink to this headline">¶</a></h1>
<p>Vocal melody transcription.</p>
<p>Transcribe vocal notes in the song and outputs the MIDI file.
Re-implementation of the work <a class="reference internal" href="#r66189d48c68f-1" id="id1">[1]</a> with tensorflow 2.3.0.
Some changes have also been made to improve the performance.</p>
<div class="section" id="feature-storage-format">
<h2>Feature Storage Format<a class="headerlink" href="#feature-storage-format" title="Permalink to this headline">¶</a></h2>
<p>Processed feature will be stored in <code class="docutils literal notranslate"><span class="pre">.hdf</span></code> file format, one file per piece.</p>
<p>Columns in the file are:</p>
<ul class="simple">
<li><p><strong>feature</strong>: CFP feature specialized for <code class="docutils literal notranslate"><span class="pre">vocal</span></code> module.</p></li>
<li><p><strong>label</strong>: Onset, offset, and duration information of the vocal.</p></li>
</ul>
<div class="section" id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<dl class="citation">
<dt class="label" id="r66189d48c68f-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/B05901022/VOCANO">https://github.com/B05901022/VOCANO</a></p>
</dd>
</dl>
</div>
<div class="section" id="see-also">
<h3>See Also<a class="headerlink" href="#see-also" title="Permalink to this headline">¶</a></h3>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">omnizart.feature.cfp.extract_vocal_cfp</span></code>:</dt><dd><p>Function to extract specialized CFP feature for <code class="docutils literal notranslate"><span class="pre">vocal</span></code>.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="app">
<h2>App<a class="headerlink" href="#app" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="omnizart.vocal.app.VocalTranscription">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.vocal.app.</code><code class="sig-name descname">VocalTranscription</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">conf_path</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.app.VocalTranscription" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../base.html#omnizart.base.BaseTranscription" title="omnizart.base.BaseTranscription"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnizart.base.BaseTranscription</span></code></a></p>
<p>Application class for vocal note transcription.</p>
<p>This application implements the training procedure in a semi-supervised way.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.vocal.app.VocalTranscription.generate_feature" title="omnizart.vocal.app.VocalTranscription.generate_feature"><code class="xref py py-obj docutils literal notranslate"><span class="pre">generate_feature</span></code></a>(dataset_path[, …])</p></td>
<td><p>Extract the feature of the whole dataset.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.vocal.app.VocalTranscription.get_model" title="omnizart.vocal.app.VocalTranscription.get_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model</span></code></a>(settings)</p></td>
<td><p>Get the Pyramid model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.vocal.app.VocalTranscription.train" title="omnizart.vocal.app.VocalTranscription.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>(feature_folder[, semi_feature_folder, …])</p></td>
<td><p>Model training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.vocal.app.VocalTranscription.transcribe" title="omnizart.vocal.app.VocalTranscription.transcribe"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transcribe</span></code></a>(input_audio[, model_path, output])</p></td>
<td><p>Transcribe vocal notes in the audio.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.vocal.app.VocalTranscription.generate_feature">
<code class="sig-name descname">generate_feature</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset_path</span></em>, <em class="sig-param"><span class="n">vocal_settings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_threads</span><span class="o">=</span><span class="default_value">4</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.app.VocalTranscription.generate_feature" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the feature of the whole dataset.</p>
<p>Currently supports MIR-1K and TONAS datasets. To train the model, you have to prepare the training
data first, then process it into feature representations. After downloading the dataset,
use this function to do the pre-processing and transform the raw data into features.</p>
<p>To specify the output path, modify the attribute
<code class="docutils literal notranslate"><span class="pre">vocal_settings.dataset.feature_save_path</span></code> to the value you want.
It will default to the folder under where the dataset stored, generating
two folders: <code class="docutils literal notranslate"><span class="pre">train_feature</span></code> and <code class="docutils literal notranslate"><span class="pre">test_feature</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>dataset_path: Path</strong></dt><dd><p>Path to the downloaded dataset.</p>
</dd>
<dt><strong>vocal_settings: VocalSettings</strong></dt><dd><p>The configuration instance that holds all relative settings for
the life-cycle of building a model.</p>
</dd>
<dt><strong>num_threads:</strong></dt><dd><p>Number of threads for parallel extracting the features.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.vocal.app.VocalTranscription.get_model">
<code class="sig-name descname">get_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">settings</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.app.VocalTranscription.get_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Pyramid model.</p>
<p>More comprehensive reasons to having this method, please refer to
<code class="docutils literal notranslate"><span class="pre">omnizart.base.BaseTranscription.get_model</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt id="omnizart.vocal.app.VocalTranscription.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature_folder</span></em>, <em class="sig-param"><span class="n">semi_feature_folder</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model_name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_model_path</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">vocal_settings</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.app.VocalTranscription.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Model training.</p>
<p>Train a new model or continue to train on a previously trained model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>feature_folder: Path</strong></dt><dd><p>Path to the folder containing generated feature.</p>
</dd>
<dt><strong>semi_feature_folder: Path</strong></dt><dd><p>If specified, semi-supervise learning will be leveraged, and the feature
files contained in this folder will be used as unsupervised data.</p>
</dd>
<dt><strong>model_name: str</strong></dt><dd><p>The name for storing the trained model. If not given, will default to the
current timesamp.</p>
</dd>
<dt><strong>input_model_path: Path</strong></dt><dd><p>Continue to train on the pre-trained model by specifying the path.</p>
</dd>
<dt><strong>vocal_settings: VocalSettings</strong></dt><dd><p>The configuration instance that holds all relative settings for
the life-cycle of building a model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.vocal.app.VocalTranscription.transcribe">
<code class="sig-name descname">transcribe</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_audio</span></em>, <em class="sig-param"><span class="n">model_path</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output</span><span class="o">=</span><span class="default_value">'./'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.app.VocalTranscription.transcribe" title="Permalink to this definition">¶</a></dt>
<dd><p>Transcribe vocal notes in the audio.</p>
<p>This function transcribes onset, offset, and pitch of the vocal in the audio.
This module is reponsible for predicting onset and offset time of each note,
and pitches are estimated by the <cite>vocal-contour</cite> submodule.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>input_audio: Path</strong></dt><dd><p>Path to the raw audio file (.wav).</p>
</dd>
<dt><strong>model_path: Path</strong></dt><dd><p>Path to the trained model or the supported transcription mode.</p>
</dd>
<dt><strong>output: Path (optional)</strong></dt><dd><p>Path for writing out the transcribed MIDI file. Default to the current path.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>midi: pretty_midi.PrettyMIDI</dt><dd><p>The transcribed vocal notes.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">omnizart.cli.vocal.transcribe</span></code></dt><dd><p>CLI entry point of this function.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">omnizart.vocal_contour.transcribe</span></code></dt><dd><p>Pitch estimation function.</p>
</dd>
</dl>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="omnizart.vocal.app.VocalDatasetLoader">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.vocal.app.</code><code class="sig-name descname">VocalDatasetLoader</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx_len</span><span class="o">=</span><span class="default_value">9</span></em>, <em class="sig-param"><span class="n">feature_folder</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">feature_files</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_samples</span><span class="o">=</span><span class="default_value">100</span></em>, <em class="sig-param"><span class="n">slice_hop</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.app.VocalDatasetLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="../base.html#omnizart.base.BaseDatasetLoader" title="omnizart.base.BaseDatasetLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">omnizart.base.BaseDatasetLoader</span></code></a></p>
<p>Dataset loader of ‘vocal’ module.</p>
<p>Defines an additional parameter ‘ctx_len’ to determine the context length
of the input feature with repect to the current timestamp.</p>
</dd></dl>

</div>
<div class="section" id="module-omnizart.vocal.inference">
<span id="inference"></span><h2>Inference<a class="headerlink" href="#module-omnizart.vocal.inference" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="omnizart.vocal.inference.infer_interval">
<code class="sig-prename descclassname">omnizart.vocal.inference.</code><code class="sig-name descname">infer_interval</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">ctx_len</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">min_dura</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">t_unit</span><span class="o">=</span><span class="default_value">0.02</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.inference.infer_interval" title="Permalink to this definition">¶</a></dt>
<dd><p>Improved version of interval inference function.</p>
<p>Inference the onset and offset time of notes given the raw prediction values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>pred:</strong></dt><dd><p>Raw prediction array.</p>
</dd>
<dt><strong>ctx_len: int</strong></dt><dd><p>Context length for determing peaks.</p>
</dd>
<dt><strong>threhsold: float</strong></dt><dd><p>Threshold for prediction values to be taken as true positive.</p>
</dd>
<dt><strong>min_dura: float</strong></dt><dd><p>Minimum duration for a note, in seconds.</p>
</dd>
<dt><strong>t_unit: float</strong></dt><dd><p>Time unit of each frame.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>interval: list[tuple[float, float]]</dt><dd><p>Pairs of inferenced onset and offset time in seconds.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.vocal.inference.infer_interval_original">
<code class="sig-prename descclassname">omnizart.vocal.inference.</code><code class="sig-name descname">infer_interval_original</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em>, <em class="sig-param"><span class="n">ctx_len</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">t_unit</span><span class="o">=</span><span class="default_value">0.02</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.inference.infer_interval_original" title="Permalink to this definition">¶</a></dt>
<dd><p>Original implementation of interval inference.</p>
<p>After checking the inference results of this implementation, we found
there are lots of missing notes that aren’t in the inferenced results.
This function is just leaving for reference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>pred:</strong></dt><dd><p>Raw prediction array.</p>
</dd>
<dt><strong>ctx_len: int</strong></dt><dd><p>Context length for determing peaks.</p>
</dd>
<dt><strong>threhsold: float</strong></dt><dd><p>Threshold for prediction values to be taken as true positive.</p>
</dd>
<dt><strong>t_unit: float</strong></dt><dd><p>Time unit of each frame.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>interval: list[tuple[float, float]]</dt><dd><p>Pairs of inferenced onset and offset time in seconds.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.vocal.inference.infer_midi">
<code class="sig-prename descclassname">omnizart.vocal.inference.</code><code class="sig-name descname">infer_midi</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">interval</span></em>, <em class="sig-param"><span class="n">agg_f0</span></em>, <em class="sig-param"><span class="n">t_unit</span><span class="o">=</span><span class="default_value">0.02</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.inference.infer_midi" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference the given interval and aggregated F0 to MIDI file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>interval: list[tuple[float, float]]</strong></dt><dd><p>The return value of <code class="docutils literal notranslate"><span class="pre">infer_interval</span></code> function. List of onset/offset pairs in seconds.</p>
</dd>
<dt><strong>agg_f0: list[dict]</strong></dt><dd><p>Aggregated f0 information. Each elements in the list should contain three columns:
<em>start_time</em>, <em>end_time</em>, and <em>frequency</em>. Time units should be in seonds, and pitch
should be Hz.</p>
</dd>
<dt><strong>t_unit: float</strong></dt><dd><p>Time unit of each frame.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>midi: pretty_midi.PrettyMIDI</dt><dd><p>The inferred MIDI object.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-omnizart.vocal.labels">
<span id="labels"></span><h2>Labels<a class="headerlink" href="#module-omnizart.vocal.labels" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="omnizart.vocal.labels.BaseLabelExtraction">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.vocal.labels.</code><code class="sig-name descname">BaseLabelExtraction</code><a class="headerlink" href="#omnizart.vocal.labels.BaseLabelExtraction" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for extract label information.</p>
<p>Provides basic functions to parse the original label format
into the target format for training.
All sub-classes should override the function <code class="docutils literal notranslate"><span class="pre">load_label</span></code>
and returns a list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Label</span></code> objects.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.vocal.labels.BaseLabelExtraction.extract_label" title="omnizart.vocal.labels.BaseLabelExtraction.extract_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extract_label</span></code></a>(label_path[, t_unit])</p></td>
<td><p>Extract SDT label.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.vocal.labels.BaseLabelExtraction.load_label" title="omnizart.vocal.labels.BaseLabelExtraction.load_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_label</span></code></a>(label_path)</p></td>
<td><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.vocal.labels.BaseLabelExtraction.extract_label">
<em class="property">classmethod </em><code class="sig-name descname">extract_label</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label_path</span></em>, <em class="sig-param"><span class="n">t_unit</span><span class="o">=</span><span class="default_value">0.02</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.labels.BaseLabelExtraction.extract_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract SDT label.</p>
<p>There are 6 types of events as defined in the original paper:
activation, silence, onset, non-onset, offset, and non-offset.
The corresponding annotations used in the paper are [a, s, o, o’, f, f’].
The ‘activation’ includes the onset and offset time. And non-onset and
non-offset events refer to when there are no onset/offset events.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>label_path: Path</strong></dt><dd><p>Path to the groun-truth file.</p>
</dd>
<dt><strong>t_unit: float</strong></dt><dd><p>Time unit of each frame.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>sdt_label: 2D numpy array</dt><dd><p>Label in SDT format with dimension: Time x 6</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.vocal.labels.BaseLabelExtraction.load_label">
<em class="property">abstract classmethod </em><code class="sig-name descname">load_label</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label_path</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.labels.BaseLabelExtraction.load_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p>
<p>Sub-classes should override this function to process their own label
format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>label_path: Path</strong></dt><dd><p>Path to the label file.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>labels: list[Label]</dt><dd><p>List of <code class="xref py py-class docutils literal notranslate"><span class="pre">Label</span></code> instances.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.vocal.labels.CMediaLabelExtraction">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.vocal.labels.</code><code class="sig-name descname">CMediaLabelExtraction</code><a class="headerlink" href="#omnizart.vocal.labels.CMediaLabelExtraction" title="Permalink to this definition">¶</a></dt>
<dd><p>Label extraction for CMedia dataset.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.vocal.labels.CMediaLabelExtraction.load_label" title="omnizart.vocal.labels.CMediaLabelExtraction.load_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_label</span></code></a>(label_path)</p></td>
<td><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.vocal.labels.CMediaLabelExtraction.load_label">
<em class="property">classmethod </em><code class="sig-name descname">load_label</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label_path</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.labels.CMediaLabelExtraction.load_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p>
<p>Sub-classes should override this function to process their own label
format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>label_path: Path</strong></dt><dd><p>Path to the label file.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>labels: list[Label]</dt><dd><p>List of <code class="xref py py-class docutils literal notranslate"><span class="pre">Label</span></code> instances.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.vocal.labels.MIR1KlabelExtraction">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.vocal.labels.</code><code class="sig-name descname">MIR1KlabelExtraction</code><a class="headerlink" href="#omnizart.vocal.labels.MIR1KlabelExtraction" title="Permalink to this definition">¶</a></dt>
<dd><p>Label extraction for MIR-1K dataset.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.vocal.labels.MIR1KlabelExtraction.load_label" title="omnizart.vocal.labels.MIR1KlabelExtraction.load_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_label</span></code></a>(label_path)</p></td>
<td><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.vocal.labels.MIR1KlabelExtraction.load_label">
<em class="property">classmethod </em><code class="sig-name descname">load_label</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label_path</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.labels.MIR1KlabelExtraction.load_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p>
<p>Sub-classes should override this function to process their own label
format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>label_path: Path</strong></dt><dd><p>Path to the label file.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>labels: list[Label]</dt><dd><p>List of <code class="xref py py-class docutils literal notranslate"><span class="pre">Label</span></code> instances.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.vocal.labels.TonasLabelExtraction">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.vocal.labels.</code><code class="sig-name descname">TonasLabelExtraction</code><a class="headerlink" href="#omnizart.vocal.labels.TonasLabelExtraction" title="Permalink to this definition">¶</a></dt>
<dd><p>Label extraction for TONAS dataset.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.vocal.labels.TonasLabelExtraction.load_label" title="omnizart.vocal.labels.TonasLabelExtraction.load_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_label</span></code></a>(label_path)</p></td>
<td><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.vocal.labels.TonasLabelExtraction.load_label">
<em class="property">classmethod </em><code class="sig-name descname">load_label</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">label_path</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.labels.TonasLabelExtraction.load_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the label file and parse information into <code class="docutils literal notranslate"><span class="pre">Label</span></code> class.</p>
<p>Sub-classes should override this function to process their own label
format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>label_path: Path</strong></dt><dd><p>Path to the label file.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>labels: list[Label]</dt><dd><p>List of <code class="xref py py-class docutils literal notranslate"><span class="pre">Label</span></code> instances.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-omnizart.vocal.prediction">
<span id="prediction"></span><h2>Prediction<a class="headerlink" href="#module-omnizart.vocal.prediction" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="omnizart.vocal.prediction.create_batches">
<code class="sig-prename descclassname">omnizart.vocal.prediction.</code><code class="sig-name descname">create_batches</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature</span></em>, <em class="sig-param"><span class="n">ctx_len</span><span class="o">=</span><span class="default_value">9</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">64</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.prediction.create_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.vocal.prediction.merge_batches">
<code class="sig-prename descclassname">omnizart.vocal.prediction.</code><code class="sig-name descname">merge_batches</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.prediction.merge_batches" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.vocal.prediction.predict">
<code class="sig-prename descclassname">omnizart.vocal.prediction.</code><code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature</span></em>, <em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">ctx_len</span><span class="o">=</span><span class="default_value">9</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">16</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.vocal.prediction.predict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="settings">
<h2>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h2>
<p>Below are the default settings for building the vocal model. It will be loaded
by the class <code class="xref py py-class docutils literal notranslate"><span class="pre">omnizart.setting_loaders.VocalSettings</span></code>. The name of the
attributes will be converted to snake-case (e.g. HopSize -&gt; hop_size). There
is also a path transformation process when applying the settings into the
<code class="docutils literal notranslate"><span class="pre">VocalSettings</span></code> instance. For example, if you want to access the attribute
<code class="docutils literal notranslate"><span class="pre">BatchSize</span></code> defined in the yaml path <em>General/Training/Settings/BatchSize</em>,
the coressponding attribute will be <em>VocalSettings.training.batch_size</em>.
The level of <em>/Settings</em> is removed among all fields.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">General</span><span class="p">:</span>
    <span class="nt">TranscriptionMode</span><span class="p">:</span>
        <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Mode of transcription by executing the `omnizart vocal transcribe` command.</span>
        <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">String</span>
        <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Semi</span>
    <span class="nt">CheckpointPath</span><span class="p">:</span>
        <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Path to the pre-trained models.</span>
        <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Map</span>
        <span class="nt">SubType</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">String</span><span class="p p-Indicator">,</span> <span class="nv">String</span><span class="p p-Indicator">]</span>
        <span class="nt">Value</span><span class="p">:</span>
            <span class="nt">Super</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">checkpoints/vocal/vocal_super</span>
            <span class="nt">Semi</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">checkpoints/vocal/vocal_semi</span>
    <span class="nt">Feature</span><span class="p">:</span>
        <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Default settings of feature extraction for drum transcription.</span>
        <span class="nt">Settings</span><span class="p">:</span>
            <span class="nt">HopSize</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Hop size in seconds with respect to sampling rate.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.02</span>
            <span class="nt">SamplingRate</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Adjust input sampling rate to this value.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">16000</span>
            <span class="nt">FrequencyResolution</span><span class="p">:</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2.0</span>
            <span class="nt">FrequencyCenter</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Lowest frequency to extract.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">80</span>
            <span class="nt">TimeCenter</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Highest frequency to extract (1/time_center).</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.001</span>
            <span class="nt">Gamma</span><span class="p">:</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">List</span>
                <span class="nt">SubType</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">0.24</span><span class="p p-Indicator">,</span> <span class="nv">0.6</span><span class="p p-Indicator">,</span> <span class="nv">1.0</span><span class="p p-Indicator">]</span>
            <span class="nt">BinsPerOctave</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Number of bins for each octave.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">48</span>
    <span class="nt">Dataset</span><span class="p">:</span>
        <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Settings of datasets.</span>
        <span class="nt">Settings</span><span class="p">:</span>
            <span class="nt">SavePath</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Path for storing the downloaded datasets.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">String</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">./</span>
            <span class="nt">FeatureSavePath</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Path for storing the extracted feature. Default to the path under the dataset folder.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">String</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">+</span>
    <span class="nt">Model</span><span class="p">:</span>
        <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Default settings of training / testing the model.</span>
        <span class="nt">Settings</span><span class="p">:</span>
            <span class="nt">SavePrefix</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Prefix of the trained model&#39;s name to be saved.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">String</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">vocal</span>
            <span class="nt">SavePath</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Path to save the trained model.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">String</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">./checkpoints/vocal</span>
            <span class="nt">MinKernelSize</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Minimum kernel size of convolution layers in each pyramid block.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">16</span>
            <span class="nt">Depth</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Total number of pyramid blocks will be -&gt; (Depth - 2) / 2 .</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">110</span>
            <span class="nt">Alpha</span><span class="p">:</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">270</span>
            <span class="nt">ShakeDrop</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Whether to leverage Shake Drop normalization when back propagation.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Bool</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>
            <span class="nt">SemiLossWeight</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Weighting factor of the semi-supervise loss. Supervised loss will not be affected by this parameter.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1.0</span>
            <span class="nt">SemiXi</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">A small constant value for weighting the adverarial perturbation.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.000001</span>
            <span class="nt">SemiEpsilon</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Weighting factor of the output adversarial perturbation.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">8.0</span>
            <span class="nt">SemiIterations</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Number of iterations when generating the adversarial perturbation.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
    <span class="nt">Inference</span><span class="p">:</span>
        <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Default settings when infering notes.</span>
        <span class="nt">Settings</span><span class="p">:</span>
            <span class="nt">ContextLength</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Length of context that will be used to find the peaks.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
            <span class="nt">Threshold</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Threshold that will be applied to clip the predicted values to either 0 or 1.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.5</span>
            <span class="nt">MinDuration</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Minimum required length of each note, in seconds.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>
            <span class="nt">PitchModel</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">The model for predicting the pitch contour. Default to use vocal-contour modeul. Could be path or mode name.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">String</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">VocalContour</span>
    <span class="nt">Training</span><span class="p">:</span>
        <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Hyper parameters for training</span>
        <span class="nt">Settings</span><span class="p">:</span>
            <span class="nt">Epoch</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Maximum number of epochs for training.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10</span>
            <span class="nt">Steps</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Number of training steps for each epoch.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1000</span>
            <span class="nt">ValSteps</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Number of validation steps after each training epoch.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">50</span>
            <span class="nt">BatchSize</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Batch size of each training step.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">64</span>
            <span class="nt">ValBatchSize</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Batch size of each validation step.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">64</span>
            <span class="nt">EarlyStop</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Terminate the training if the validation performance doesn&#39;t imrove after n epochs.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">8</span>
            <span class="nt">InitLearningRate</span><span class="p">:</span>
                <span class="nt">Descriptoin</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Initial learning rate.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Float</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.0001</span>
            <span class="nt">ContextLength</span><span class="p">:</span>
                <span class="nt">Description</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Context to be considered before and after current timestamp.</span>
                <span class="nt">Type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Integer</span>
                <span class="nt">Value</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">9</span>
</pre></div>
</div>
</div>
</div>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="../chord/api.html"
       title="previous chapter">← Chord Transcription</a>
  </li>
  <li class="next">
    <a href="../vocal-contour/api.html"
       title="next chapter">Vocal-Contour Transcription →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright 2020, MCTLab.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.4.3 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a>.
</div>
            </div>
          </div>
      </page>
    </div>
    
    
  </body>
</html>