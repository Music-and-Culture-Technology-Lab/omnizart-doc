<!DOCTYPE html>
<html >
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
      <title>Models</title>
    
      <link rel="stylesheet" href="_static/pygments.css">
      <link rel="stylesheet" href="_static/theme.css">
      <link rel="stylesheet" href="_static/sphinx_press_theme.css">
      
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>

      <!-- sphinx script_files -->
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

      
      <script src="_static/theme-vendors.js"></script>
      <script src="_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="genindex.html" />
  <link rel="search" title="Search" href="search.html" />
  <link rel="next" title="Training" href="training.html" />
  <link rel="prev" title="Feature" href="feature.html" /> 
  </head>

  <body>
    <div id="app" class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="index.html" class="home-link">
    
      <span class="site-name">omnizart</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">

  
    <div class="nav-item">
      <a href="index.html#demo"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#demo"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            

  
    <div class="nav-item">
      <a href="index.html#demo"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#demo"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#demo">Contents</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 "><a href="quick-start.html" class="reference internal ">Quick Start</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="tutorial.html" class="reference internal ">Tutorial</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="music/cli.html" class="reference internal ">omnizart music</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="drum/cli.html" class="reference internal ">omnizart drum</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="chord/cli.html" class="reference internal ">omnizart chord</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="vocal/cli.html" class="reference internal ">omnizart vocal</a>

            
          </li>

        
      </ul>
    </div>
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#demo">API Reference</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 "><a href="music/api.html" class="reference internal ">Music Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="drum/api.html" class="reference internal ">Drum Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="chord/api.html" class="reference internal ">Chord Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="vocal/api.html" class="reference internal ">Vocal Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="feature.html" class="reference internal ">Feature</a>

            
          </li>

        
          <li class="toctree-l1 current"><a href="#" class="reference internal current">Models</a>

            
              <ul>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.u_net" class="reference internal">U-Net</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.t2t" class="reference internal">Tensor2Tensor</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.spectral_norm_net" class="reference internal">Spectral Normalization Model</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.chord_model" class="reference internal">Chord Transformer</a></li>
                
                  <li class="toctree-l2"><a href="#pyramid-net" class="reference internal">Pyramid Net</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.utils" class="reference internal">Utils</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 "><a href="training.html" class="reference internal ">Training</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="base.html" class="reference internal ">Base Classes</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="constants.html" class="reference internal ">Constants</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="utils.html" class="reference internal ">Utilities</a>

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
    
    <li>Models</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="feature.html"
       title="previous chapter">← Feature</a>
  </li>
  <li class="next">
    <a href="training.html"
       title="next chapter">Training →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main">
            
  <div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-omnizart.models"></span><p>Definitions of models and corresponding layers</p>
<p>This folder contains all the definition of model architectures, including common layers that
could share among different models.</p>
<div class="section" id="module-omnizart.models.u_net">
<span id="u-net"></span><h2>U-Net<a class="headerlink" href="#module-omnizart.models.u_net" title="Permalink to this headline">¶</a></h2>
<p>Definition of customized U-Net like model architecture.</p>
<dl class="py class">
<dt id="omnizart.models.u_net.MultiHeadAttention">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">MultiHeadAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Attention layer for 2D input feature.</p>
<p>As the attention mechanism consumes a large amount of memory, here we leverage a divide-and-conquer approach
implemented in the <code class="docutils literal notranslate"><span class="pre">tensor2tensor</span></code> repository. The input feature is first partitioned into smaller parts before
being passed to do self-attention computation. The processed outputs are then assembled back into the same
size as the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>out_channel: int</strong></dt><dd><p>Number of output channels.</p>
</dd>
<dt><strong>d_model: int</strong></dt><dd><p>Dimension of embeddings for each position of input feature.</p>
</dd>
<dt><strong>n_heads: int</strong></dt><dd><p>Number of heads for multi-head attention computation. Should be division of <cite>d_model</cite>.</p>
</dd>
<dt><strong>query_shape: Tuple(int, int)</strong></dt><dd><p>Size of each partition.</p>
</dd>
<dt><strong>memory_flange: Tuple(int, int)</strong></dt><dd><p>Additional overlapping size to be extended to each partition, indicating the final size to be
computed is: (query_shape[0]+memory_flange[0]) x (query_shape[1]+memory_flange[1])</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<p>This approach is originated from <a class="reference internal" href="#r80975e0fd81b-1" id="id1">[1]</a>.</p>
<dl class="citation">
<dt class="label" id="r80975e0fd81b-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran,
“Image Transformer,” in Proceedings of the 35th International Conference on Machine Learning (ICML), 2018</p>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.u_net.MultiHeadAttention.call" title="omnizart.models.u_net.MultiHeadAttention.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.u_net.MultiHeadAttention.get_config" title="omnizart.models.u_net.MultiHeadAttention.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.u_net.MultiHeadAttention.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.u_net.MultiHeadAttention.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.conv_block">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">channel</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">strides</span><span class="o">=</span><span class="default_value">(2, 2)</span></em>, <em class="sig-param"><span class="n">dilation_rate</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">dropout_rate</span><span class="o">=</span><span class="default_value">0.4</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.conv_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Convolutional encoder block of U-net.</p>
<p>The block is a fully convolutional block. The encoder block does not downsample the input feature,
and thus the output will have the same dimension as the input.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.semantic_segmentation">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">semantic_segmentation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature_num</span><span class="o">=</span><span class="default_value">352</span></em>, <em class="sig-param"><span class="n">timesteps</span><span class="o">=</span><span class="default_value">256</span></em>, <em class="sig-param"><span class="n">multi_grid_layer_n</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">multi_grid_n</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">ch_num</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">out_class</span><span class="o">=</span><span class="default_value">2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.semantic_segmentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Improved U-net model with Atrous Spatial Pyramid Pooling (ASPP) block.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.semantic_segmentation_attn">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">semantic_segmentation_attn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature_num</span><span class="o">=</span><span class="default_value">352</span></em>, <em class="sig-param"><span class="n">timesteps</span><span class="o">=</span><span class="default_value">256</span></em>, <em class="sig-param"><span class="n">ch_num</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">out_class</span><span class="o">=</span><span class="default_value">2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.semantic_segmentation_attn" title="Permalink to this definition">¶</a></dt>
<dd><p>Customized attention U-net model.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.transpose_conv_block">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">transpose_conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">channel</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">strides</span><span class="o">=</span><span class="default_value">(2, 2)</span></em>, <em class="sig-param"><span class="n">dropout_rate</span><span class="o">=</span><span class="default_value">0.4</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.transpose_conv_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-omnizart.models.t2t">
<span id="tensor2tensor"></span><h2>Tensor2Tensor<a class="headerlink" href="#module-omnizart.models.t2t" title="Permalink to this headline">¶</a></h2>
<p>Implementation of memory efficient attention.</p>
<p>Original implemetation are from <a class="reference external" href="https://github.com/tensorflow/tensor2tensor">tensor2tensor</a>.
Rewrite in tensorflow 2.0.</p>
<dl class="py class">
<dt id="omnizart.models.t2t.MultiHeadAttention">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">MultiHeadAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-head attention keras layer wrapper</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.t2t.MultiHeadAttention.call" title="omnizart.models.t2t.MultiHeadAttention.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(q, k, v)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.t2t.MultiHeadAttention.get_config" title="omnizart.models.t2t.MultiHeadAttention.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.t2t.MultiHeadAttention.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">v</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.MultiHeadAttention.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id6"><span class="problematic" id="id7">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.t2t.MultiHeadAttention.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.MultiHeadAttention.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.cast_like">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">cast_like</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.cast_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Cast x to y’s dtype, if necessary.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.combine_heads_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">combine_heads_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.combine_heads_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Inverse of split_heads_2d.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape [batch, num_heads, height, width, channels / num_heads]</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape [batch, height, width, channels]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.combine_last_two_dimensions">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">combine_last_two_dimensions</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.combine_last_two_dimensions" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape x so that the last two dimension become one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape […, a, b]</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape […, ab]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.dot_product_attention">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">dot_product_attention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">v</span></em>, <em class="sig-param"><span class="n">bias</span></em>, <em class="sig-param"><span class="n">dropout_rate</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">save_weights_to</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dropout_broadcast_dims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">activation_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_dtype</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.dot_product_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Dot-product attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>q</strong></dt><dd><p>Tensor with shape […, length_q, depth_k].</p>
</dd>
<dt><strong>k</strong></dt><dd><p>Tensor with shape […, length_kv, depth_k]. Leading dimensions must
match with q.</p>
</dd>
<dt><strong>v</strong></dt><dd><p>Tensor with shape […, length_kv, depth_v] Leading dimensions must
match with q.</p>
</dd>
<dt><strong>bias</strong></dt><dd><p>Bias Tensor (see attention_bias())</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of layers.</p>
</dd>
<dt><strong>image_shapes: tuple</strong></dt><dd><p>Optional tuple of integer scalars.</p>
</dd>
<dt><strong>name: str</strong></dt><dd><p>An optional string</p>
</dd>
<dt><strong>save_weights_to: dict</strong></dt><dd><p>An optional dictionary to capture attention weights
for visualization; the weights tensor will be appended there under
a string key created from the variable scope (including name).</p>
</dd>
<dt><strong>dropout_broadcast_dims: list</strong></dt><dd><p>An optional list of integers less than rank of q.
Specifies in which dimensions to broadcast the dropout decisions.</p>
</dd>
<dt><strong>activation_dtype:</strong></dt><dd><p>Used to define function activation dtype when using
mixed precision.</p>
</dd>
<dt><strong>weight_dtype:</strong></dt><dd><p>The dtype weights are stored in when using mixed precision</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>Tensor with shape […, length_q, depth_v].</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.dropout_with_broadcast_dims">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">dropout_with_broadcast_dims</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">keep_prob</span></em>, <em class="sig-param"><span class="n">broadcast_dims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.dropout_with_broadcast_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>Like tf.nn.dropout but takes broadcast_dims instead of noise_shape.</p>
<p>Instead of specifying noise_shape, this function takes broadcast_dims -
a list of dimension numbers in which noise_shape should be 1.  The random
keep/drop tensor has dimensionality 1 along these dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x: float</strong></dt><dd><p>A floating point tensor.</p>
</dd>
<dt><strong>keep_prob</strong></dt><dd><p>A scalar Tensor with the same type as x.
The probability that each element is kept.</p>
</dd>
<dt><strong>broadcast_dims: int</strong></dt><dd><p>An optional list of integers
the dimensions along which to broadcast the keep/drop flags.</p>
</dd>
<dt><strong>**kwargs</strong></dt><dd><p>keyword arguments to tf.nn.dropout other than “noise_shape”.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>Tensor of the same shape as x.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.embedding_to_padding">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">embedding_to_padding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">emb</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.embedding_to_padding" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the padding mask based on which embeddings are all zero.</p>
<p>We have hacked symbol_modality to return all-zero embeddings for padding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>emb:</strong></dt><dd><p>A Tensor with shape […, depth].</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A float Tensor with shape […]. Each element is 1 if its corresponding
embedding vector is all zero, and is 0 otherwise.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.gather_blocks_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">gather_blocks_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">indices</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.gather_blocks_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers flattened blocks from x.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.gather_indices_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">gather_indices_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">block_shape</span></em>, <em class="sig-param"><span class="n">block_stride</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.gather_indices_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Getting gather indices.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.local_attention_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">local_attention_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">v</span></em>, <em class="sig-param"><span class="n">query_shape</span><span class="o">=</span><span class="default_value">(8, 16)</span></em>, <em class="sig-param"><span class="n">memory_flange</span><span class="o">=</span><span class="default_value">(8, 16)</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.local_attention_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Strided block local self-attention.</p>
<p>The 2-D sequence is divided into 2-D blocks of shape query_shape. Attention
for a given query position can only see memory positions less than or equal to
the query position. The memory positions are the corresponding block with
memory_flange many positions to add to the height and width of the block
(namely, left, top, and right).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>q</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_k]</p>
</dd>
<dt><strong>k</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_k]</p>
</dd>
<dt><strong>v</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_v]. In the current
implementation, depth_v must be equal to depth_k.</p>
</dd>
<dt><strong>query_shape: tuple</strong></dt><dd><p>An tuple indicating the height and width of each query block.</p>
</dd>
<dt><strong>memory_flange: tuple</strong></dt><dd><p>An integer indicating how much to look in height and width
from each query block.</p>
</dd>
<dt><strong>name: str</strong></dt><dd><p>An optional string</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor of shape [batch, heads, h, w, depth_v]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.maybe_upcast">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">maybe_upcast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logits</span></em>, <em class="sig-param"><span class="n">activation_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hparams</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.maybe_upcast" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.mixed_precision_is_enabled">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">mixed_precision_is_enabled</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">activation_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hparams</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.mixed_precision_is_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.pad_to_multiple_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">pad_to_multiple_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">block_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.pad_to_multiple_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Making sure x is a multiple of shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A [batch, heads, h, w, depth] or [batch, h, w, depth] tensor</p>
</dd>
<dt><strong>block_shape</strong></dt><dd><p>A 2D list of integer shapes</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>padded_x</dt><dd><p>A [batch, heads, h, w, depth] or [batch, h, w, depth] tensor</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.positional_encoding">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">positional_encoding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span></em>, <em class="sig-param"><span class="n">timesteps</span></em>, <em class="sig-param"><span class="n">n_units</span><span class="o">=</span><span class="default_value">512</span></em>, <em class="sig-param"><span class="n">zero_pad</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">scale</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.positional_encoding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.relative_positional_encoding">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">relative_positional_encoding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">n_steps</span></em>, <em class="sig-param"><span class="n">n_units</span><span class="o">=</span><span class="default_value">512</span></em>, <em class="sig-param"><span class="n">max_dist</span><span class="o">=</span><span class="default_value">2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.relative_positional_encoding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.reshape_range">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">reshape_range</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">j</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.reshape_range" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshapes a tensor between dimensions i and j.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.scatter_blocks_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">scatter_blocks_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.scatter_blocks_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>scatters blocks from x into shape with indices.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.split_heads_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">split_heads_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">num_heads</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.split_heads_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Split channels (dimension 3) into multiple heads (becomes dimension 1).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A tensor with shape [batch, height, width, channels]</p>
</dd>
<dt><strong>num_heads: int</strong></dt><dd><p>Number of heads in attention’s computation.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A tensor with shape [batch, num_heads, height, width, channels / num_heads]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.split_last_dimension">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">split_last_dimension</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">n</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.split_last_dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape x so that the last dimension becomes two dimensions.</p>
<p>The first of these two dimensions is n.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape […, m]</p>
</dd>
<dt><strong>n: int</strong></dt><dd><p>An integer.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape […, n, m/n]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-omnizart.models.spectral_norm_net">
<span id="spectral-normalization-model"></span><h2>Spectral Normalization Model<a class="headerlink" href="#module-omnizart.models.spectral_norm_net" title="Permalink to this headline">¶</a></h2>
<p>Transcription model of drum leveraging spectral normalization.</p>
<p>The model was originally developed with tensorflow 1.12.
We rewrite the model with tensorflow 2.3 module and uses keras to implement most of
the functionalities for better readability.</p>
<p>Original Author: I-Chieh, Wei
Rewrite by: BreezeWhite</p>
<dl class="py class">
<dt id="omnizart.models.spectral_norm_net.ConvSN2D">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">ConvSN2D</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Just a wrapper layer for using spectral normalization.</p>
<p>Original implementation referes to <a class="reference external" href="https://github.com/thisisiron/spectral_normalization-tf2">here</a>.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.ConvSN2D.call" title="omnizart.models.spectral_norm_net.ConvSN2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.ConvSN2D.get_config" title="omnizart.models.spectral_norm_net.ConvSN2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>This is neccessary to save the model architecture.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.ConvSN2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id8"><span class="problematic" id="id9">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.ConvSN2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>This is neccessary to save the model architecture.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">SpectralNormalization</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.layers.wrappers.Wrapper</span></code></p>
<p>Spectral normalization layer.</p>
<p>Original implementation referes to <a class="reference external" href="https://github.com/thisisiron/spectral_normalization-tf2">here</a>.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.SpectralNormalization.build" title="omnizart.models.spectral_norm_net.SpectralNormalization.build"><code class="xref py py-obj docutils literal notranslate"><span class="pre">build</span></code></a>(input_shape)</p></td>
<td><p>Creates the variables of the layer (optional, for subclass implementers).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.SpectralNormalization.call" title="omnizart.models.spectral_norm_net.SpectralNormalization.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%" />
<col style="width: 34%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>restore_weights</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>update_weights</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id11"><span class="problematic" id="id12">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.restore_weights">
<code class="sig-name descname">restore_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.restore_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.update_weights">
<code class="sig-name descname">update_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.update_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.cnn_attention">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">cnn_attention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'attention'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.cnn_attention" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.conv_sa">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">conv_sa</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">kernel</span><span class="o">=</span><span class="default_value">(4, 4)</span></em>, <em class="sig-param"><span class="n">strides</span><span class="o">=</span><span class="default_value">(2, 2)</span></em>, <em class="sig-param"><span class="n">pad</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">pad_type</span><span class="o">=</span><span class="default_value">'zero'</span></em>, <em class="sig-param"><span class="n">spectral_norm</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'conv_0'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.conv_sa" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.down_sample">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">down_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.down_sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.drum_model">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">drum_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out_classes</span></em>, <em class="sig-param"><span class="n">mini_beat_per_seg</span></em>, <em class="sig-param"><span class="n">res_block_num</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">channels</span><span class="o">=</span><span class="default_value">64</span></em>, <em class="sig-param"><span class="n">spectral_norm</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.drum_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the drum transcription model.</p>
<p>Constructs the drum transcription model instance for training/inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>out_classes: int</strong></dt><dd><p>Total output classes, refering to classes of drum types.
Currently there are 13 pre-defined drum percussions.</p>
</dd>
<dt><strong>mini_beat_per_seg: int</strong></dt><dd><p>Number of mini beats in a segment. Can be understood as the range of time
to be considered for training.</p>
</dd>
<dt><strong>res_block_num: int</strong></dt><dd><p>Number of residual blocks.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>model: tf.keras.Model</dt><dd><p>A tensorflow keras model instance.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.residual_block">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">residual_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">spectral_norm</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'resblock'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.residual_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.transpose_residual_block">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">transpose_residual_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">to_down</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">spectral_norm</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'transblock'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.transpose_residual_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-omnizart.models.chord_model">
<span id="chord-transformer"></span><h2>Chord Transformer<a class="headerlink" href="#module-omnizart.models.chord_model" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="omnizart.models.chord_model.ChordModel">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">ChordModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.training.Model</span></code></p>
<p>Chord model in written in keras.</p>
<p>Keras model of <code class="docutils literal notranslate"><span class="pre">chord</span></code> submodule. The original implementation is written in
tensorflow 1.11 and can be found <a class="reference external" href="https://github.com/Tsung-Ping/Harmony-Transformer">here</a>.</p>
<p>The model also implements the custom training/test step due to the specialized loss
computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>num_enc_attn_blocks: int</strong></dt><dd><p>Number of attention blocks in the encoder.</p>
</dd>
<dt><strong>num_dec_attn_blocks: int</strong></dt><dd><p>Number of attention blocks in the decoder.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>out_classes: int</strong></dt><dd><p>Number of output classes. Currently supports 26 types of chords.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>enc_input_emb_size: int</strong></dt><dd><p>Embedding size of the encoder’s input.</p>
</dd>
<dt><strong>dec_input_emb_size: int</strong></dt><dd><p>Embedding size of the decoder’s input.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all the dropout layers.</p>
</dd>
<dt><strong>annealing_rate: float</strong></dt><dd><p>Rate of modifying the slope value for each epoch.</p>
</dd>
<dt><strong>**kwargs:</strong></dt><dd><p>Other keyword parameters that will be passed to initialize the keras.Model.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">omnizart.chord.app.chord_loss_func</span></code></dt><dd><p>The customized loss computation function.</p>
</dd>
</dl>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.call" title="omnizart.models.chord_model.ChordModel.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(feature)</p></td>
<td><p>Calls the model on new inputs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.get_config" title="omnizart.models.chord_model.ChordModel.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.test_step" title="omnizart.models.chord_model.ChordModel.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(data)</p></td>
<td><p>The logic for one evaluation step.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.train_step" title="omnizart.models.chord_model.ChordModel.train_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_step</span></code></a>(data)</p></td>
<td><p>The logic for one training step.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 63%" />
<col style="width: 37%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>step_in_slope</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.chord_model.ChordModel.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<dl>
<dt>Arguments:</dt><dd><p>inputs: A tensor or list of tensors.
training: Boolean or boolean scalar tensor, indicating whether to run</p>
<blockquote>
<div><p>the <cite>Network</cite> in training mode or inference mode.</p>
</div></blockquote>
<dl class="simple">
<dt>mask: A mask or list of masks. A mask can be</dt><dd><p>either a tensor or None (no mask).</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.ChordModel.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.ChordModel.step_in_slope">
<code class="sig-name descname">step_in_slope</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.step_in_slope" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.ChordModel.test_step">
<code class="sig-name descname">test_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one evaluation step.</p>
<p>This method can be overridden to support custom evaluation logic.
This method is called by <cite>Model.make_test_function</cite>.</p>
<p>This function should contain the mathemetical logic for one step of
evaluation.
This typically includes the forward pass, loss calculation, and metrics
updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_test_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id14"><span class="problematic" id="id15">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.ChordModel.train_step">
<code class="sig-name descname">train_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.train_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <cite>Model.make_train_function</cite>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_train_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id16"><span class="problematic" id="id17">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned. Example:
<cite>{‘loss’: 0.2, ‘accuracy’: 0.7}</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.chord_model.Decoder">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">Decoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Decoder layer of the transformer model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>out_classes: int</strong></dt><dd><p>Number of output classes. Currently supports 26 types of chords.</p>
</dd>
<dt><strong>num_attn_blocks:</strong></dt><dd><p>Number of attention blocks.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>dec_input_emb_size: int</strong></dt><dd><p>Embedding size of the decoder’s input.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all the dropout layers.</p>
</dd>
<dt><strong>**kwargs:</strong></dt><dd><p>Other keyword parameters that will be passed to initialize keras.layers.Layer.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Decoder.call" title="omnizart.models.chord_model.Decoder.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp, encoder_input_emb, chord_change_pred)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Decoder.get_config" title="omnizart.models.chord_model.Decoder.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.chord_model.Decoder.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inp</span></em>, <em class="sig-param"><span class="n">encoder_input_emb</span></em>, <em class="sig-param"><span class="n">chord_change_pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Decoder.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id18"><span class="problematic" id="id19">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.Decoder.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Decoder.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.chord_model.EncodeSegmentFrequency">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">EncodeSegmentFrequency</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentFrequency" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Encode feature along the frequency axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_units: int</strong></dt><dd><p>Output embedding size.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all dropout layers.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentFrequency.call" title="omnizart.models.chord_model.EncodeSegmentFrequency.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentFrequency.get_config" title="omnizart.models.chord_model.EncodeSegmentFrequency.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.chord_model.EncodeSegmentFrequency.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inp</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentFrequency.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id20"><span class="problematic" id="id21">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.EncodeSegmentFrequency.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentFrequency.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.chord_model.EncodeSegmentTime">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">EncodeSegmentTime</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentTime" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Encode feature along the time axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_units: int</strong></dt><dd><p>Output embedding size.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all dropout layers.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentTime.call" title="omnizart.models.chord_model.EncodeSegmentTime.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentTime.get_config" title="omnizart.models.chord_model.EncodeSegmentTime.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.chord_model.EncodeSegmentTime.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inp</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentTime.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id22"><span class="problematic" id="id23">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.EncodeSegmentTime.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentTime.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.chord_model.Encoder">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">Encoder</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Encoder layer of the transformer model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>num_attn_blocks:</strong></dt><dd><p>Number of attention blocks.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>enc_input_emb_size: int</strong></dt><dd><p>Embedding size of the encoder’s input.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all the dropout layers.</p>
</dd>
<dt><strong>**kwargs:</strong></dt><dd><p>Other keyword parameters that will be passed to initialize keras.layers.Layer.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Encoder.call" title="omnizart.models.chord_model.Encoder.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp[, slope])</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Encoder.get_config" title="omnizart.models.chord_model.Encoder.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.chord_model.Encoder.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inp</span></em>, <em class="sig-param"><span class="n">slope</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Encoder.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id24"><span class="problematic" id="id25">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.Encoder.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Encoder.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.chord_model.FeedForward">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">FeedForward</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.FeedForward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Feedfoward layer of the transformer model.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.FeedForward.call" title="omnizart.models.chord_model.FeedForward.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.FeedForward.get_config" title="omnizart.models.chord_model.FeedForward.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.chord_model.FeedForward.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inp</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.FeedForward.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id26"><span class="problematic" id="id27">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.chord_model.FeedForward.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.FeedForward.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.chord_model.ReduceSlope">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">ReduceSlope</code><a class="headerlink" href="#omnizart.models.chord_model.ReduceSlope" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.callbacks.Callback</span></code></p>
<p>Custom keras callback for reducing slope value after each epoch.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ReduceSlope.on_epoch_end" title="omnizart.models.chord_model.ReduceSlope.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>(epoch[, logs])</p></td>
<td><p>Called at the end of an epoch.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.chord_model.ReduceSlope.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span></em>, <em class="sig-param"><span class="n">logs</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ReduceSlope.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<dl>
<dt>Arguments:</dt><dd><p>epoch: Integer, index of epoch.
logs: Dict, metric results for this training epoch, and for the</p>
<blockquote>
<div><p>validation epoch if validation is performed. Validation result keys
are prefixed with <cite>val_</cite>.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="omnizart.models.chord_model.binary_round">
<code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">binary_round</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inp</span></em>, <em class="sig-param"><span class="n">cast_to_int</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.binary_round" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.chord_model.chord_block_compression">
<code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">chord_block_compression</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hidden_states</span></em>, <em class="sig-param"><span class="n">chord_changes</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.chord_block_compression" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.chord_model.chord_block_decompression">
<code class="sig-prename descclassname">omnizart.models.chord_model.</code><code class="sig-name descname">chord_block_decompression</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">compressed_seq</span></em>, <em class="sig-param"><span class="n">block_ids</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.chord_block_decompression" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="pyramid-net">
<h2>Pyramid Net<a class="headerlink" href="#pyramid-net" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-omnizart.models.utils">
<span id="utils"></span><h2>Utils<a class="headerlink" href="#module-omnizart.models.utils" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="omnizart.models.utils.get_contour">
<code class="sig-prename descclassname">omnizart.models.utils.</code><code class="sig-name descname">get_contour</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.utils.get_contour" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.utils.note_res_downsampling">
<code class="sig-prename descclassname">omnizart.models.utils.</code><code class="sig-name descname">note_res_downsampling</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">score</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.utils.note_res_downsampling" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.utils.padding">
<code class="sig-prename descclassname">omnizart.models.utils.</code><code class="sig-name descname">padding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">feature_num</span></em>, <em class="sig-param"><span class="n">timesteps</span></em>, <em class="sig-param"><span class="n">dimension</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.utils.padding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.utils.shape_list">
<code class="sig-prename descclassname">omnizart.models.utils.</code><code class="sig-name descname">shape_list</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.utils.shape_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Return list of dims, statically where possible.</p>
</dd></dl>

</div>
</div>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="feature.html"
       title="previous chapter">← Feature</a>
  </li>
  <li class="next">
    <a href="training.html"
       title="next chapter">Training →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright 2020, MCTLab.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.3.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a>.
</div>
            </div>
          </div>
      </page>
    </div>
    
    
  </body>
</html>