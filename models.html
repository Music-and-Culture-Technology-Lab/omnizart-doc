<!DOCTYPE html>
<html >
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
      <title>Models</title>
    
      <link rel="stylesheet" href="_static/pygments.css">
      <link rel="stylesheet" href="_static/theme.css">
      <link rel="stylesheet" href="_static/sphinx_press_theme.css">
      
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>

      <!-- sphinx script_files -->
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

      
      <script src="_static/theme-vendors.js"></script>
      <script src="_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="genindex.html" />
  <link rel="search" title="Search" href="search.html" />
  <link rel="next" title="Constants" href="constants.html" />
  <link rel="prev" title="Feature" href="feature.html" /> 
  </head>

  <body>
    <div id="app" class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="index.html" class="home-link">
    
      <span class="site-name">omnizart</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">

  
    <div class="nav-item">
      <a href="index.html#omniscient-mozart-s-document"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#omniscient-mozart-s-document"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            

  
    <div class="nav-item">
      <a href="index.html#omniscient-mozart-s-document"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#omniscient-mozart-s-document"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#omniscient-mozart-s-document">Contents</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 "><a href="quick-start.html" class="reference internal ">Quick Start</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="cli.html" class="reference internal ">Command Line Interface</a>

            
          </li>

        
      </ul>
    </div>
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#omniscient-mozart-s-document">API Reference</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 "><a href="music.html" class="reference internal ">Music Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="drum.html" class="reference internal ">Drum Transcription</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="base.html" class="reference internal ">Base Classes</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="feature.html" class="reference internal ">Feature</a>

            
          </li>

        
          <li class="toctree-l1 current"><a href="#" class="reference internal current">Models</a>

            
              <ul>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.u_net" class="reference internal">U-Net</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.t2t" class="reference internal">Tensor2Tensor</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.spectral_norm_net" class="reference internal">Spectral Normalization Model</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.utils" class="reference internal">Utils</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 "><a href="constants.html" class="reference internal ">Constants</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="utils.html" class="reference internal ">Utilities</a>

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
    
    <li>Models</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="feature.html"
       title="previous chapter">← Feature</a>
  </li>
  <li class="next">
    <a href="constants.html"
       title="next chapter">Constants →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main">
            
  <div class="section" id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-omnizart.models"></span><p>Definitions of models and corresponding layers</p>
<p>This folder contains all the definition of model architectures, including common layers that
could share among different models.</p>
<div class="section" id="module-omnizart.models.u_net">
<span id="u-net"></span><h2>U-Net<a class="headerlink" href="#module-omnizart.models.u_net" title="Permalink to this headline">¶</a></h2>
<p>Definition of customized U-Net like model architecture.</p>
<dl class="py class">
<dt id="omnizart.models.u_net.MultiHeadAttention">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">MultiHeadAttention</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Attention layer for 2D input feature.</p>
<p>As the attention mechanism consumes a large amount of memory, here we leverage a divide-and-conquer approach
implemented in the <code class="docutils literal notranslate"><span class="pre">tensor2tensor</span></code> repository. The input feature is first partitioned into smaller parts before
being passed to do self-attention computation. The processed outputs are then assembled back into the same
size as the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>out_channel: int</strong></dt><dd><p>Number of output channels.</p>
</dd>
<dt><strong>d_model: int</strong></dt><dd><p>Dimension of embeddings for each position of input feature.</p>
</dd>
<dt><strong>n_heads: int</strong></dt><dd><p>Number of heads for multi-head attention computation. Should be division of <cite>d_model</cite>.</p>
</dd>
<dt><strong>query_shape: Tuple(int, int)</strong></dt><dd><p>Size of each partition.</p>
</dd>
<dt><strong>memory_flange: Tuple(int, int)</strong></dt><dd><p>Additional overlapping size to be extended to each partition, indicating the final size to be
computed is: (query_shape[0]+memory_flange[0]) x (query_shape[1]+memory_flange[1])</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<p>This approach is originated from <a class="reference internal" href="#r80975e0fd81b-1" id="id1">[1]</a>.</p>
<dl class="citation">
<dt class="label" id="r80975e0fd81b-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran,
“Image Transformer,” in Proceedings of the 35th International Conference on Machine Learning (ICML), 2018</p>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.u_net.MultiHeadAttention.call" title="omnizart.models.u_net.MultiHeadAttention.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.u_net.MultiHeadAttention.get_config" title="omnizart.models.u_net.MultiHeadAttention.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.u_net.MultiHeadAttention.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.u_net.MultiHeadAttention.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.conv_block">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">channel</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">strides</span><span class="o">=</span><span class="default_value">2, 2</span></em>, <em class="sig-param"><span class="n">dilation_rate</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">dropout_rate</span><span class="o">=</span><span class="default_value">0.4</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.conv_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Convolutional encoder block of U-net.</p>
<p>The block is a fully convolutional block. The encoder block does not downsample the input feature,
and thus the output will have the same dimension as the input.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.semantic_segmentation">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">semantic_segmentation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature_num</span><span class="o">=</span><span class="default_value">352</span></em>, <em class="sig-param"><span class="n">timesteps</span><span class="o">=</span><span class="default_value">256</span></em>, <em class="sig-param"><span class="n">multi_grid_layer_n</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">multi_grid_n</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">ch_num</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">out_class</span><span class="o">=</span><span class="default_value">2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.semantic_segmentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Improved U-net model with Atrous Spatial Pyramid Pooling (ASPP) block.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.semantic_segmentation_attn">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">semantic_segmentation_attn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">feature_num</span><span class="o">=</span><span class="default_value">352</span></em>, <em class="sig-param"><span class="n">timesteps</span><span class="o">=</span><span class="default_value">256</span></em>, <em class="sig-param"><span class="n">ch_num</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">out_class</span><span class="o">=</span><span class="default_value">2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.semantic_segmentation_attn" title="Permalink to this definition">¶</a></dt>
<dd><p>Customized attention U-net model.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.u_net.transpose_conv_block">
<code class="sig-prename descclassname">omnizart.models.u_net.</code><code class="sig-name descname">transpose_conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em>, <em class="sig-param"><span class="n">channel</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">strides</span><span class="o">=</span><span class="default_value">2, 2</span></em>, <em class="sig-param"><span class="n">dropout_rate</span><span class="o">=</span><span class="default_value">0.4</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.transpose_conv_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-omnizart.models.t2t">
<span id="tensor2tensor"></span><h2>Tensor2Tensor<a class="headerlink" href="#module-omnizart.models.t2t" title="Permalink to this headline">¶</a></h2>
<p>Implementation of memory efficient attention.</p>
<p>Original implemetation are from <a class="reference external" href="https://github.com/tensorflow/tensor2tensor">tensor2tensor</a>.
Rewrite in tensorflow 2.0.</p>
<dl class="py function">
<dt id="omnizart.models.t2t.cast_like">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">cast_like</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.cast_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Cast x to y’s dtype, if necessary.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.combine_heads_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">combine_heads_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.combine_heads_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Inverse of split_heads_2d.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape [batch, num_heads, height, width, channels / num_heads]</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape [batch, height, width, channels]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.combine_last_two_dimensions">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">combine_last_two_dimensions</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.combine_last_two_dimensions" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape x so that the last two dimension become one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape […, a, b]</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape […, ab]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.dot_product_attention">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">dot_product_attention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">v</span></em>, <em class="sig-param"><span class="n">bias</span></em>, <em class="sig-param"><span class="n">dropout_rate</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">save_weights_to</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dropout_broadcast_dims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">activation_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_dtype</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.dot_product_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Dot-product attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>q</strong></dt><dd><p>Tensor with shape […, length_q, depth_k].</p>
</dd>
<dt><strong>k</strong></dt><dd><p>Tensor with shape […, length_kv, depth_k]. Leading dimensions must
match with q.</p>
</dd>
<dt><strong>v</strong></dt><dd><p>Tensor with shape […, length_kv, depth_v] Leading dimensions must
match with q.</p>
</dd>
<dt><strong>bias</strong></dt><dd><p>Bias Tensor (see attention_bias())</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of layers.</p>
</dd>
<dt><strong>image_shapes: tuple</strong></dt><dd><p>Optional tuple of integer scalars.</p>
</dd>
<dt><strong>name: str</strong></dt><dd><p>An optional string</p>
</dd>
<dt><strong>save_weights_to: dict</strong></dt><dd><p>An optional dictionary to capture attention weights
for visualization; the weights tensor will be appended there under
a string key created from the variable scope (including name).</p>
</dd>
<dt><strong>dropout_broadcast_dims: list</strong></dt><dd><p>An optional list of integers less than rank of q.
Specifies in which dimensions to broadcast the dropout decisions.</p>
</dd>
<dt><strong>activation_dtype:</strong></dt><dd><p>Used to define function activation dtype when using
mixed precision.</p>
</dd>
<dt><strong>weight_dtype:</strong></dt><dd><p>The dtype weights are stored in when using mixed precision</p>
</dd>
<dt><strong>hard_attention_k: int</strong></dt><dd><p>If &gt; 0 triggers hard attention (picking top-k)
hard_attention_k &lt;= 0.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>Tensor with shape […, length_q, depth_v].</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.dropout_with_broadcast_dims">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">dropout_with_broadcast_dims</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">keep_prob</span></em>, <em class="sig-param"><span class="n">broadcast_dims</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.dropout_with_broadcast_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>Like tf.nn.dropout but takes broadcast_dims instead of noise_shape.</p>
<p>Instead of specifying noise_shape, this function takes broadcast_dims -
a list of dimension numbers in which noise_shape should be 1.  The random
keep/drop tensor has dimensionality 1 along these dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x: float</strong></dt><dd><p>A floating point tensor.</p>
</dd>
<dt><strong>keep_prob</strong></dt><dd><p>A scalar Tensor with the same type as x.
The probability that each element is kept.</p>
</dd>
<dt><strong>broadcast_dims: int</strong></dt><dd><p>An optional list of integers
the dimensions along which to broadcast the keep/drop flags.</p>
</dd>
<dt><strong>**kwargs</strong></dt><dd><p>keyword arguments to tf.nn.dropout other than “noise_shape”.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>Tensor of the same shape as x.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.embedding_to_padding">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">embedding_to_padding</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">emb</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.embedding_to_padding" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the padding mask based on which embeddings are all zero.</p>
<p>We have hacked symbol_modality to return all-zero embeddings for padding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>emb:</strong></dt><dd><p>A Tensor with shape […, depth].</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A float Tensor with shape […]. Each element is 1 if its corresponding
embedding vector is all zero, and is 0 otherwise.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.gather_blocks_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">gather_blocks_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">indices</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.gather_blocks_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers flattened blocks from x.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.gather_indices_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">gather_indices_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">block_shape</span></em>, <em class="sig-param"><span class="n">block_stride</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.gather_indices_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Getting gather indices.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.local_attention_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">local_attention_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">q</span></em>, <em class="sig-param"><span class="n">k</span></em>, <em class="sig-param"><span class="n">v</span></em>, <em class="sig-param"><span class="n">query_shape</span><span class="o">=</span><span class="default_value">8, 16</span></em>, <em class="sig-param"><span class="n">memory_flange</span><span class="o">=</span><span class="default_value">8, 16</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.local_attention_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Strided block local self-attention.</p>
<p>The 2-D sequence is divided into 2-D blocks of shape query_shape. Attention
for a given query position can only see memory positions less than or equal to
the query position. The memory positions are the corresponding block with
memory_flange many positions to add to the height and width of the block
(namely, left, top, and right).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>q</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_k]</p>
</dd>
<dt><strong>k</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_k]</p>
</dd>
<dt><strong>v</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_v]. In the current
implementation, depth_v must be equal to depth_k.</p>
</dd>
<dt><strong>query_shape: tuple</strong></dt><dd><p>An tuple indicating the height and width of each query block.</p>
</dd>
<dt><strong>memory_flange: tuple</strong></dt><dd><p>An integer indicating how much to look in height and width
from each query block.</p>
</dd>
<dt><strong>name: str</strong></dt><dd><p>An optional string</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor of shape [batch, heads, h, w, depth_v]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.maybe_upcast">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">maybe_upcast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logits</span></em>, <em class="sig-param"><span class="n">activation_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hparams</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.maybe_upcast" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.mixed_precision_is_enabled">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">mixed_precision_is_enabled</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">activation_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_dtype</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">hparams</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.mixed_precision_is_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.pad_to_multiple_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">pad_to_multiple_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">block_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.pad_to_multiple_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Making sure x is a multiple of shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A [batch, heads, h, w, depth] or [batch, h, w, depth] tensor</p>
</dd>
<dt><strong>block_shape</strong></dt><dd><p>A 2D list of integer shapes</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>padded_x</dt><dd><p>A [batch, heads, h, w, depth] or [batch, h, w, depth] tensor</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.reshape_range">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">reshape_range</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span></em>, <em class="sig-param"><span class="n">i</span></em>, <em class="sig-param"><span class="n">j</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.reshape_range" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshapes a tensor between dimensions i and j.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.scatter_blocks_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">scatter_blocks_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">indices</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.scatter_blocks_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>scatters blocks from x into shape with indices.</p>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.split_heads_2d">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">split_heads_2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">num_heads</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.split_heads_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Split channels (dimension 3) into multiple heads (becomes dimension 1).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A tensor with shape [batch, height, width, channels]</p>
</dd>
<dt><strong>num_heads: int</strong></dt><dd><p>Number of heads in attention’s computation.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A tensor with shape [batch, num_heads, height, width, channels / num_heads]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="omnizart.models.t2t.split_last_dimension">
<code class="sig-prename descclassname">omnizart.models.t2t.</code><code class="sig-name descname">split_last_dimension</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">n</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.split_last_dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape x so that the last dimension becomes two dimensions.</p>
<p>The first of these two dimensions is n.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape […, m]</p>
</dd>
<dt><strong>n: int</strong></dt><dd><p>An integer.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape […, n, m/n]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-omnizart.models.spectral_norm_net">
<span id="spectral-normalization-model"></span><h2>Spectral Normalization Model<a class="headerlink" href="#module-omnizart.models.spectral_norm_net" title="Permalink to this headline">¶</a></h2>
<p>Transcription model of drum, which leverages spectral normalization.</p>
<p>The model was originally developed with tensorflow 1.12.
We rewrite the model with tensorflow 2.3 module and uses keras to implement most of
the functionalities for better readaility.</p>
<p>Original Author: I-Chieh, Wei
Rewrite by: BreezeWhite</p>
<dl class="py class">
<dt id="omnizart.models.spectral_norm_net.ConvSN2D">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">ConvSN2D</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Just a wrapper layer for using spectral normalization.</p>
<p>Original implementation referes to <a class="reference external" href="https://github.com/thisisiron/spectral_normalization-tf2">here</a>.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.ConvSN2D.call" title="omnizart.models.spectral_norm_net.ConvSN2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.ConvSN2D.get_config" title="omnizart.models.spectral_norm_net.ConvSN2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>This is neccessary to save the model architecture.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.ConvSN2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id6"><span class="problematic" id="id7">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.ConvSN2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>This is neccessary to save the model architecture.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization">
<em class="property">class </em><code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">SpectralNormalization</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.layers.wrappers.Wrapper</span></code></p>
<p>Spectral normalization layer.</p>
<p>Original implementation referes to <a class="reference external" href="https://github.com/thisisiron/spectral_normalization-tf2">here</a>.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.SpectralNormalization.build" title="omnizart.models.spectral_norm_net.SpectralNormalization.build"><code class="xref py py-obj docutils literal notranslate"><span class="pre">build</span></code></a>(input_shape)</p></td>
<td><p>Creates the variables of the layer (optional, for subclass implementers).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.SpectralNormalization.call" title="omnizart.models.spectral_norm_net.SpectralNormalization.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%" />
<col style="width: 34%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>restore_weights</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>update_weights</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.build">
<code class="sig-name descname">build</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id9"><span class="problematic" id="id10">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.restore_weights">
<code class="sig-name descname">restore_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.restore_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="omnizart.models.spectral_norm_net.SpectralNormalization.update_weights">
<code class="sig-name descname">update_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.update_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.cnn_attention">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">cnn_attention</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'attention'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.cnn_attention" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.conv_sa">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">conv_sa</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">kernel</span><span class="o">=</span><span class="default_value">4, 4</span></em>, <em class="sig-param"><span class="n">strides</span><span class="o">=</span><span class="default_value">2, 2</span></em>, <em class="sig-param"><span class="n">pad</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">pad_type</span><span class="o">=</span><span class="default_value">'zero'</span></em>, <em class="sig-param"><span class="n">spectral_norm</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'conv_0'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.conv_sa" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.down_sample">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">down_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.down_sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.drum_model">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">drum_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out_classes</span></em>, <em class="sig-param"><span class="n">mini_beat_per_seg</span></em>, <em class="sig-param"><span class="n">layer_num</span><span class="o">=</span><span class="default_value">3</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.drum_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.residual_block">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">residual_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">spectral_norm</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'resblock'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.residual_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="omnizart.models.spectral_norm_net.transpose_residual_block">
<code class="sig-prename descclassname">omnizart.models.spectral_norm_net.</code><code class="sig-name descname">transpose_residual_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em>, <em class="sig-param"><span class="n">channels</span></em>, <em class="sig-param"><span class="n">to_down</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">spectral_norm</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">scope</span><span class="o">=</span><span class="default_value">'transblock'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.transpose_residual_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-omnizart.models.utils">
<span id="utils"></span><h2>Utils<a class="headerlink" href="#module-omnizart.models.utils" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="omnizart.models.utils.shape_list">
<code class="sig-prename descclassname">omnizart.models.utils.</code><code class="sig-name descname">shape_list</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_tensor</span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.utils.shape_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Return list of dims, statically where possible.</p>
</dd></dl>

</div>
</div>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="feature.html"
       title="previous chapter">← Feature</a>
  </li>
  <li class="next">
    <a href="constants.html"
       title="next chapter">Constants →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright 2020, BreezeWhite.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a>.
</div>
            </div>
          </div>
      </page>
    </div>
    
    
  </body>
</html>