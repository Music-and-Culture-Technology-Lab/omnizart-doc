<!DOCTYPE html>
<html >
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

      <title>Models</title>
    
          <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
          <link rel="stylesheet" href="_static/css/waveform-list.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="_static/theme-vendors.js"></script> -->
      <script src="_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="genindex.html" />
  <link rel="search" title="Search" href="search.html" />
  <link rel="next" title="Training" href="training.html" />
  <link rel="prev" title="Feature" href="feature.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="index.html" class="home-link">
    
      <span class="site-name">omnizart</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">

  
    <div class="nav-item">
      <a href="index.html#sound-samples"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#sound-samples"
         class="nav-link ">
         Command Line Interface
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#sound-samples"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            

  
    <div class="nav-item">
      <a href="index.html#sound-samples"
         class="nav-link ">
         Contents
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#sound-samples"
         class="nav-link ">
         Command Line Interface
      </a>
    </div>
  
    <div class="nav-item">
      <a href="index.html#sound-samples"
         class="nav-link  router-link-active">
         API Reference
      </a>
    </div>
  



            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#sound-samples">Contents</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 ">
            
              <a href="quick-start.html" class="reference internal ">Quick Start</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="tutorial.html" class="reference internal ">Tutorial</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="demo.html" class="reference internal ">Demonstration</a>
            

            
          </li>

        
      </ul>
    </div>
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#sound-samples">Command Line Interface</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 ">
            
              <a href="music/cli.html" class="reference internal ">omnizart music</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="drum/cli.html" class="reference internal ">omnizart drum</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="chord/cli.html" class="reference internal ">omnizart chord</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="vocal/cli.html" class="reference internal ">omnizart vocal</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="vocal-contour/cli.html" class="reference internal ">omnizart vocal-contour</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="beat/cli.html" class="reference internal ">omnizart beat</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="patch-cnn/cli.html" class="reference internal ">omnizart patch-cnn</a>
            

            
          </li>

        
      </ul>
    </div>
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#sound-samples">API Reference</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="music/api.html" class="reference internal ">Music Transcription</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="drum/api.html" class="reference internal ">Drum Transcription</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="chord/api.html" class="reference internal ">Chord Transcription</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="vocal/api.html" class="reference internal ">Vocal Transcription</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="vocal-contour/api.html" class="reference internal ">Vocal-Contour Transcription</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="patch-cnn/api.html" class="reference internal ">Patch-CNN Transcription</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="beat/api.html" class="reference internal ">Beat Transcription</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="feature.html" class="reference internal ">Feature</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Models</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.u_net" class="reference internal">U-Net</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.t2t" class="reference internal">Tensor2Tensor</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.spectral_norm_net" class="reference internal">Spectral Normalization Model</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.chord_model" class="reference internal">Chord Transformer</a></li>
                
                  <li class="toctree-l2"><a href="#pyramid-net" class="reference internal">Pyramid Net</a></li>
                
                  <li class="toctree-l2"><a href="#module-omnizart.models.utils" class="reference internal">Utils</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="training.html" class="reference internal ">Training</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="base.html" class="reference internal ">Base Classes</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="constants.html" class="reference internal ">Constants</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="utils.html" class="reference internal ">Utilities</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
    
    <li>Models</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="feature.html"
       title="previous chapter">← Feature</a>
  </li>
  <li class="next">
    <a href="training.html"
       title="next chapter">Training →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section id="models">
<h1>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-omnizart.models"></span><p>Definitions of models and corresponding layers</p>
<p>This folder contains all the definition of model architectures, including common layers that
could share among different models.</p>
<section id="module-omnizart.models.u_net">
<span id="u-net"></span><h2>U-Net<a class="headerlink" href="#module-omnizart.models.u_net" title="Permalink to this headline">¶</a></h2>
<p>Definition of customized U-Net like model architecture.</p>
<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.u_net.MultiHeadAttention">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.u_net.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Attention layer for 2D input feature.</p>
<p>As the attention mechanism consumes a large amount of memory, here we leverage a divide-and-conquer approach
implemented in the <code class="docutils literal notranslate"><span class="pre">tensor2tensor</span></code> repository. The input feature is first partitioned into smaller parts before
being passed to do self-attention computation. The processed outputs are then assembled back into the same
size as the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>out_channel: int</strong></dt><dd><p>Number of output channels.</p>
</dd>
<dt><strong>d_model: int</strong></dt><dd><p>Dimension of embeddings for each position of input feature.</p>
</dd>
<dt><strong>n_heads: int</strong></dt><dd><p>Number of heads for multi-head attention computation. Should be division of <cite>d_model</cite>.</p>
</dd>
<dt><strong>query_shape: Tuple(int, int)</strong></dt><dd><p>Size of each partition.</p>
</dd>
<dt><strong>memory_flange: Tuple(int, int)</strong></dt><dd><p>Additional overlapping size to be extended to each partition, indicating the final size to be
computed is: (query_shape[0]+memory_flange[0]) x (query_shape[1]+memory_flange[1])</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<p>This approach is originated from <a class="reference internal" href="#r80975e0fd81b-1" id="id1">[1]</a>.</p>
<dl class="citation">
<dt class="label" id="r80975e0fd81b-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran,
“Image Transformer,” in Proceedings of the 35th International Conference on Machine Learning (ICML), 2018</p>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.u_net.MultiHeadAttention.call" title="omnizart.models.u_net.MultiHeadAttention.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.u_net.MultiHeadAttention.get_config" title="omnizart.models.u_net.MultiHeadAttention.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.u_net.MultiHeadAttention.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.u_net.MultiHeadAttention.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.MultiHeadAttention.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.u_net.conv_block">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.u_net.</span></span><span class="sig-name descname"><span class="pre">conv_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.4</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.conv_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Convolutional encoder block of U-net.</p>
<p>The block is a fully convolutional block. The encoder block does not downsample the input feature,
and thus the output will have the same dimension as the input.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.u_net.semantic_segmentation">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.u_net.</span></span><span class="sig-name descname"><span class="pre">semantic_segmentation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feature_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">352</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timesteps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_grid_layer_n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_grid_n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ch_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.4</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.semantic_segmentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Improved U-net model with Atrous Spatial Pyramid Pooling (ASPP) block.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.u_net.semantic_segmentation_attn">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.u_net.</span></span><span class="sig-name descname"><span class="pre">semantic_segmentation_attn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feature_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">352</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timesteps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ch_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.semantic_segmentation_attn" title="Permalink to this definition">¶</a></dt>
<dd><p>Customized attention U-net model.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.u_net.transpose_conv_block">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.u_net.</span></span><span class="sig-name descname"><span class="pre">transpose_conv_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.4</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.u_net.transpose_conv_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-omnizart.models.t2t">
<span id="tensor2tensor"></span><h2>Tensor2Tensor<a class="headerlink" href="#module-omnizart.models.t2t" title="Permalink to this headline">¶</a></h2>
<p>Implementation of memory efficient attention.</p>
<p>Original implemetation are from <a class="reference external" href="https://github.com/tensorflow/tensor2tensor">tensor2tensor</a>.
Rewrite in tensorflow 2.0.</p>
<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.t2t.MultiHeadAttention">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Multi-head attention keras layer wrapper</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.t2t.MultiHeadAttention.call" title="omnizart.models.t2t.MultiHeadAttention.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(q, k, v)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.t2t.MultiHeadAttention.get_config" title="omnizart.models.t2t.MultiHeadAttention.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.t2t.MultiHeadAttention.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.MultiHeadAttention.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id6"><span class="problematic" id="id7">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.t2t.MultiHeadAttention.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.MultiHeadAttention.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.cast_like">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">cast_like</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.cast_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Cast x to y’s dtype, if necessary.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.combine_heads_2d">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">combine_heads_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.combine_heads_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Inverse of split_heads_2d.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape [batch, num_heads, height, width, channels / num_heads]</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape [batch, height, width, channels]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.combine_last_two_dimensions">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">combine_last_two_dimensions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.combine_last_two_dimensions" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape x so that the last two dimension become one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape […, a, b]</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape […, ab]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.dot_product_attention">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">dot_product_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_weights_to</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.dot_product_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Dot-product attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>q</strong></dt><dd><p>Tensor with shape […, length_q, depth_k].</p>
</dd>
<dt><strong>k</strong></dt><dd><p>Tensor with shape […, length_kv, depth_k]. Leading dimensions must
match with q.</p>
</dd>
<dt><strong>v</strong></dt><dd><p>Tensor with shape […, length_kv, depth_v] Leading dimensions must
match with q.</p>
</dd>
<dt><strong>bias</strong></dt><dd><p>Bias Tensor (see attention_bias())</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of layers.</p>
</dd>
<dt><strong>image_shapes: tuple</strong></dt><dd><p>Optional tuple of integer scalars.</p>
</dd>
<dt><strong>name: str</strong></dt><dd><p>An optional string</p>
</dd>
<dt><strong>save_weights_to: dict</strong></dt><dd><p>An optional dictionary to capture attention weights
for visualization; the weights tensor will be appended there under
a string key created from the variable scope (including name).</p>
</dd>
<dt><strong>dropout_broadcast_dims: list</strong></dt><dd><p>An optional list of integers less than rank of q.
Specifies in which dimensions to broadcast the dropout decisions.</p>
</dd>
<dt><strong>activation_dtype:</strong></dt><dd><p>Used to define function activation dtype when using
mixed precision.</p>
</dd>
<dt><strong>weight_dtype:</strong></dt><dd><p>The dtype weights are stored in when using mixed precision</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>Tensor with shape […, length_q, depth_v].</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.dropout_with_broadcast_dims">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">dropout_with_broadcast_dims</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_prob</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcast_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.dropout_with_broadcast_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>Like tf.nn.dropout but takes broadcast_dims instead of noise_shape.</p>
<p>Instead of specifying noise_shape, this function takes broadcast_dims -
a list of dimension numbers in which noise_shape should be 1.  The random
keep/drop tensor has dimensionality 1 along these dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x: float</strong></dt><dd><p>A floating point tensor.</p>
</dd>
<dt><strong>keep_prob</strong></dt><dd><p>A scalar Tensor with the same type as x.
The probability that each element is kept.</p>
</dd>
<dt><strong>broadcast_dims: int</strong></dt><dd><p>An optional list of integers
the dimensions along which to broadcast the keep/drop flags.</p>
</dd>
<dt><strong>**kwargs</strong></dt><dd><p>keyword arguments to tf.nn.dropout other than “noise_shape”.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>Tensor of the same shape as x.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.embedding_to_padding">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">embedding_to_padding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">emb</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.embedding_to_padding" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the padding mask based on which embeddings are all zero.</p>
<p>We have hacked symbol_modality to return all-zero embeddings for padding.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>emb:</strong></dt><dd><p>A Tensor with shape […, depth].</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A float Tensor with shape […]. Each element is 1 if its corresponding
embedding vector is all zero, and is 0 otherwise.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.gather_blocks_2d">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">gather_blocks_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.gather_blocks_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers flattened blocks from x.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.gather_indices_2d">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">gather_indices_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_stride</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.gather_indices_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Getting gather indices.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.local_attention_2d">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">local_attention_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">query_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(8,</span> <span class="pre">16)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_flange</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(8,</span> <span class="pre">16)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.local_attention_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Strided block local self-attention.</p>
<p>The 2-D sequence is divided into 2-D blocks of shape query_shape. Attention
for a given query position can only see memory positions less than or equal to
the query position. The memory positions are the corresponding block with
memory_flange many positions to add to the height and width of the block
(namely, left, top, and right).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>q</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_k]</p>
</dd>
<dt><strong>k</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_k]</p>
</dd>
<dt><strong>v</strong></dt><dd><p>A tensor with shape [batch, heads, h, w, depth_v]. In the current
implementation, depth_v must be equal to depth_k.</p>
</dd>
<dt><strong>query_shape: tuple</strong></dt><dd><p>An tuple indicating the height and width of each query block.</p>
</dd>
<dt><strong>memory_flange: tuple</strong></dt><dd><p>An integer indicating how much to look in height and width
from each query block.</p>
</dd>
<dt><strong>name: str</strong></dt><dd><p>An optional string</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor of shape [batch, heads, h, w, depth_v]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.maybe_upcast">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">maybe_upcast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.maybe_upcast" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.mixed_precision_is_enabled">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">mixed_precision_is_enabled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.mixed_precision_is_enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.pad_to_multiple_2d">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">pad_to_multiple_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.pad_to_multiple_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Making sure x is a multiple of shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A [batch, heads, h, w, depth] or [batch, h, w, depth] tensor</p>
</dd>
<dt><strong>block_shape</strong></dt><dd><p>A 2D list of integer shapes</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>padded_x</dt><dd><p>A [batch, heads, h, w, depth] or [batch, h, w, depth] tensor</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.positional_encoding">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">positional_encoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timesteps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_units</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.positional_encoding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.relative_positional_encoding">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">relative_positional_encoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_units</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.relative_positional_encoding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.reshape_range">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">reshape_range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">j</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.reshape_range" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshapes a tensor between dimensions i and j.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.scatter_blocks_2d">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">scatter_blocks_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.scatter_blocks_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>scatters blocks from x into shape with indices.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.split_heads_2d">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">split_heads_2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.split_heads_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Split channels (dimension 3) into multiple heads (becomes dimension 1).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A tensor with shape [batch, height, width, channels]</p>
</dd>
<dt><strong>num_heads: int</strong></dt><dd><p>Number of heads in attention’s computation.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A tensor with shape [batch, num_heads, height, width, channels / num_heads]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.t2t.split_last_dimension">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.t2t.</span></span><span class="sig-name descname"><span class="pre">split_last_dimension</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.t2t.split_last_dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Reshape x so that the last dimension becomes two dimensions.</p>
<p>The first of these two dimensions is n.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong></dt><dd><p>A Tensor with shape […, m]</p>
</dd>
<dt><strong>n: int</strong></dt><dd><p>An integer.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>y</dt><dd><p>A Tensor with shape […, n, m/n]</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-omnizart.models.spectral_norm_net">
<span id="spectral-normalization-model"></span><h2>Spectral Normalization Model<a class="headerlink" href="#module-omnizart.models.spectral_norm_net" title="Permalink to this headline">¶</a></h2>
<p>Transcription model of drum leveraging spectral normalization.</p>
<p>The model was originally developed with tensorflow 1.12.
We rewrite the model with tensorflow 2.3 module and uses keras to implement most of
the functionalities for better readability.</p>
<p>Original Author: I-Chieh, Wei
Rewrite by: BreezeWhite</p>
<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.ConvSN2D">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">ConvSN2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Just a wrapper layer for using spectral normalization.</p>
<p>Original implementation referes to <a class="reference external" href="https://github.com/thisisiron/spectral_normalization-tf2">here</a>.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.ConvSN2D.call" title="omnizart.models.spectral_norm_net.ConvSN2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.ConvSN2D.get_config" title="omnizart.models.spectral_norm_net.ConvSN2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>This is neccessary to save the model architecture.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.ConvSN2D.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id8"><span class="problematic" id="id9">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.ConvSN2D.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.ConvSN2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>This is neccessary to save the model architecture.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.SpectralNormalization">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">SpectralNormalization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.layers.wrappers.Wrapper</span></code></p>
<p>Spectral normalization layer.</p>
<p>Original implementation referes to <a class="reference external" href="https://github.com/thisisiron/spectral_normalization-tf2">here</a>.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.SpectralNormalization.build" title="omnizart.models.spectral_norm_net.SpectralNormalization.build"><code class="xref py py-obj docutils literal notranslate"><span class="pre">build</span></code></a>(input_shape)</p></td>
<td><p>Creates the variables of the layer (optional, for subclass implementers).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.spectral_norm_net.SpectralNormalization.call" title="omnizart.models.spectral_norm_net.SpectralNormalization.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 66%" />
<col style="width: 34%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>restore_weights</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>update_weights</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.SpectralNormalization.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.SpectralNormalization.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id11"><span class="problematic" id="id12">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.SpectralNormalization.restore_weights">
<span class="sig-name descname"><span class="pre">restore_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.restore_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.SpectralNormalization.update_weights">
<span class="sig-name descname"><span class="pre">update_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.SpectralNormalization.update_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.cnn_attention">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">cnn_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scope</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'attention'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.cnn_attention" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.conv_sa">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">conv_sa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(4,</span> <span class="pre">4)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zero'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spectral_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scope</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'conv_0'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.conv_sa" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.down_sample">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">down_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.down_sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.drum_model">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">drum_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mini_beat_per_seg</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">res_block_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spectral_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.drum_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the drum transcription model.</p>
<p>Constructs the drum transcription model instance for training/inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>out_classes: int</strong></dt><dd><p>Total output classes, refering to classes of drum types.
Currently there are 13 pre-defined drum percussions.</p>
</dd>
<dt><strong>mini_beat_per_seg: int</strong></dt><dd><p>Number of mini beats in a segment. Can be understood as the range of time
to be considered for training.</p>
</dd>
<dt><strong>res_block_num: int</strong></dt><dd><p>Number of residual blocks.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>model: tf.keras.Model</dt><dd><p>A tensorflow keras model instance.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.residual_block">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">residual_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spectral_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scope</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'resblock'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.residual_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.spectral_norm_net.transpose_residual_block">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.spectral_norm_net.</span></span><span class="sig-name descname"><span class="pre">transpose_residual_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_down</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spectral_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scope</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'transblock'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.spectral_norm_net.transpose_residual_block" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-omnizart.models.chord_model">
<span id="chord-transformer"></span><h2>Chord Transformer<a class="headerlink" href="#module-omnizart.models.chord_model" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ChordModel">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">ChordModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.training.Model</span></code></p>
<p>Chord model in written in keras.</p>
<p>Keras model of <code class="docutils literal notranslate"><span class="pre">chord</span></code> submodule. The original implementation is written in
tensorflow 1.11 and can be found <a class="reference external" href="https://github.com/Tsung-Ping/Harmony-Transformer">here</a>.</p>
<p>The model also implements the custom training/test step due to the specialized loss
computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>num_enc_attn_blocks: int</strong></dt><dd><p>Number of attention blocks in the encoder.</p>
</dd>
<dt><strong>num_dec_attn_blocks: int</strong></dt><dd><p>Number of attention blocks in the decoder.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>out_classes: int</strong></dt><dd><p>Number of output classes. Currently supports 26 types of chords.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>enc_input_emb_size: int</strong></dt><dd><p>Embedding size of the encoder’s input.</p>
</dd>
<dt><strong>dec_input_emb_size: int</strong></dt><dd><p>Embedding size of the decoder’s input.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all the dropout layers.</p>
</dd>
<dt><strong>annealing_rate: float</strong></dt><dd><p>Rate of modifying the slope value for each epoch.</p>
</dd>
<dt><strong>**kwargs:</strong></dt><dd><p>Other keyword parameters that will be passed to initialize the keras.Model.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">omnizart.chord.app.chord_loss_func</span></code></dt><dd><p>The customized loss computation function.</p>
</dd>
</dl>
</div>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.call" title="omnizart.models.chord_model.ChordModel.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(feature)</p></td>
<td><p>Calls the model on new inputs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.get_config" title="omnizart.models.chord_model.ChordModel.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.test_step" title="omnizart.models.chord_model.ChordModel.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(data)</p></td>
<td><p>The logic for one evaluation step.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ChordModel.train_step" title="omnizart.models.chord_model.ChordModel.train_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_step</span></code></a>(data)</p></td>
<td><p>The logic for one training step.</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<colgroup>
<col style="width: 63%" />
<col style="width: 37%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>step_in_slope</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ChordModel.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feature</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<dl>
<dt>Arguments:</dt><dd><p>inputs: A tensor or list of tensors.
training: Boolean or boolean scalar tensor, indicating whether to run</p>
<blockquote>
<div><p>the <cite>Network</cite> in training mode or inference mode.</p>
</div></blockquote>
<dl class="simple">
<dt>mask: A mask or list of masks. A mask can be</dt><dd><p>either a tensor or None (no mask).</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ChordModel.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ChordModel.step_in_slope">
<span class="sig-name descname"><span class="pre">step_in_slope</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.step_in_slope" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ChordModel.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one evaluation step.</p>
<p>This method can be overridden to support custom evaluation logic.
This method is called by <cite>Model.make_test_function</cite>.</p>
<p>This function should contain the mathemetical logic for one step of
evaluation.
This typically includes the forward pass, loss calculation, and metrics
updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_test_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id14"><span class="problematic" id="id15">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ChordModel.train_step">
<span class="sig-name descname"><span class="pre">train_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ChordModel.train_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
This method is called by <cite>Model.make_train_function</cite>.</p>
<p>This method should contain the mathemetical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_train_function</cite>, which can also be overridden.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>data: A nested structure of <a href="#id16"><span class="problematic" id="id17">`</span></a>Tensor`s.</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned. Example:
<cite>{‘loss’: 0.2, ‘accuracy’: 0.7}</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.chord_model.Decoder">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Decoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Decoder layer of the transformer model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>out_classes: int</strong></dt><dd><p>Number of output classes. Currently supports 26 types of chords.</p>
</dd>
<dt><strong>num_attn_blocks:</strong></dt><dd><p>Number of attention blocks.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>dec_input_emb_size: int</strong></dt><dd><p>Embedding size of the decoder’s input.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all the dropout layers.</p>
</dd>
<dt><strong>**kwargs:</strong></dt><dd><p>Other keyword parameters that will be passed to initialize keras.layers.Layer.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Decoder.call" title="omnizart.models.chord_model.Decoder.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp, encoder_input_emb, chord_change_pred)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Decoder.get_config" title="omnizart.models.chord_model.Decoder.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.Decoder.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_input_emb</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chord_change_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Decoder.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id18"><span class="problematic" id="id19">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.Decoder.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Decoder.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.chord_model.EncodeSegmentFrequency">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">EncodeSegmentFrequency</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentFrequency" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Encode feature along the frequency axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_units: int</strong></dt><dd><p>Output embedding size.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all dropout layers.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentFrequency.call" title="omnizart.models.chord_model.EncodeSegmentFrequency.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentFrequency.get_config" title="omnizart.models.chord_model.EncodeSegmentFrequency.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.EncodeSegmentFrequency.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentFrequency.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id20"><span class="problematic" id="id21">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.EncodeSegmentFrequency.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentFrequency.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.chord_model.EncodeSegmentTime">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">EncodeSegmentTime</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentTime" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Encode feature along the time axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_units: int</strong></dt><dd><p>Output embedding size.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all dropout layers.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentTime.call" title="omnizart.models.chord_model.EncodeSegmentTime.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.EncodeSegmentTime.get_config" title="omnizart.models.chord_model.EncodeSegmentTime.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.EncodeSegmentTime.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentTime.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id22"><span class="problematic" id="id23">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.EncodeSegmentTime.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.EncodeSegmentTime.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.chord_model.Encoder">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">Encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Encoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Encoder layer of the transformer model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>num_attn_blocks:</strong></dt><dd><p>Number of attention blocks.</p>
</dd>
<dt><strong>n_steps: int</strong></dt><dd><p>Time length of the feature.</p>
</dd>
<dt><strong>enc_input_emb_size: int</strong></dt><dd><p>Embedding size of the encoder’s input.</p>
</dd>
<dt><strong>segment_width: int</strong></dt><dd><p>Context width of each frame. Nearby frames will be concatenated to the feature axis.
Default to 21, which means past 10 frames and future 10 frames will be concatenated
to the current frame, resulting a feature dimenstion of <em>segment_width x freq_size</em>.</p>
</dd>
<dt><strong>freq_size: int</strong></dt><dd><p>Feature size of the input representation.</p>
</dd>
<dt><strong>dropout_rate: float</strong></dt><dd><p>Dropout rate of all the dropout layers.</p>
</dd>
<dt><strong>**kwargs:</strong></dt><dd><p>Other keyword parameters that will be passed to initialize keras.layers.Layer.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Encoder.call" title="omnizart.models.chord_model.Encoder.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp[, slope])</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.Encoder.get_config" title="omnizart.models.chord_model.Encoder.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.Encoder.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">slope</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Encoder.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id24"><span class="problematic" id="id25">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.Encoder.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.Encoder.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.chord_model.FeedForward">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">FeedForward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.FeedForward" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Feedfoward layer of the transformer model.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.FeedForward.call" title="omnizart.models.chord_model.FeedForward.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inp)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#omnizart.models.chord_model.FeedForward.get_config" title="omnizart.models.chord_model.FeedForward.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.FeedForward.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.FeedForward.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id26"><span class="problematic" id="id27">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.FeedForward.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.FeedForward.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ReduceSlope">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">ReduceSlope</span></span><a class="headerlink" href="#omnizart.models.chord_model.ReduceSlope" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.callbacks.Callback</span></code></p>
<p>Custom keras callback for reducing slope value after each epoch.</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#omnizart.models.chord_model.ReduceSlope.on_epoch_end" title="omnizart.models.chord_model.ReduceSlope.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>(epoch[, logs])</p></td>
<td><p>Called at the end of an epoch.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="omnizart.models.chord_model.ReduceSlope.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.ReduceSlope.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<dl>
<dt>Arguments:</dt><dd><p>epoch: Integer, index of epoch.
logs: Dict, metric results for this training epoch, and for the</p>
<blockquote>
<div><p>validation epoch if validation is performed. Validation result keys
are prefixed with <cite>val_</cite>.</p>
</div></blockquote>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.chord_model.binary_round">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">binary_round</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cast_to_int</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.binary_round" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.chord_model.chord_block_compression">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">chord_block_compression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chord_changes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.chord_block_compression" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.chord_model.chord_block_decompression">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.chord_model.</span></span><span class="sig-name descname"><span class="pre">chord_block_decompression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">compressed_seq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_ids</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.chord_model.chord_block_decompression" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="pyramid-net">
<h2>Pyramid Net<a class="headerlink" href="#pyramid-net" title="Permalink to this headline">¶</a></h2>
</section>
<section id="module-omnizart.models.utils">
<span id="utils"></span><h2>Utils<a class="headerlink" href="#module-omnizart.models.utils" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="omnizart.models.utils.shape_list">
<span class="sig-prename descclassname"><span class="pre">omnizart.models.utils.</span></span><span class="sig-name descname"><span class="pre">shape_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#omnizart.models.utils.shape_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Return list of dims, statically where possible.</p>
</dd></dl>

</section>
</section>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="feature.html"
       title="previous chapter">← Feature</a>
  </li>
  <li class="next">
    <a href="training.html"
       title="next chapter">Training →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright 2020, MCTLab.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.8.0.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>